{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SparkSQL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JhonFiUNFV/python_prep/blob/master/SparkSQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQgCzt-8UrH6"
      },
      "source": [
        "# 1.- Configuracion Entorno Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdKw7_n8UEX9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6590fb76-842c-4ce3-8412-273cb53a3d47"
      },
      "source": [
        "# Descargamos spark con hadoop y Java\n",
        "!apt-get update # Actualizar el gestor de paquetes de Linux\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null # Descargar Java\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-2.4.8/spark-2.4.8-bin-hadoop2.7.tgz # Descargar Apache Spark\n",
        "!tar xf spark-2.4.8-bin-hadoop2.7.tgz # Descomprimir Spark\n",
        "!pip install -q findspark # Para utilizar Spark"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwhBfxJaUq-Z"
      },
      "source": [
        "# Seteamos las variables de entorno\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.7-bin-hadoop2.7\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgrBPLJeVJzO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "outputId": "623f9f18-4e30-4017-a075-7677793c1eae"
      },
      "source": [
        "# Creamos la conexion a Spark \n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/findspark.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mpy4j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lib\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"py4j-*.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-5d58e7b434f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Creamos la conexion a Spark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfindspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local[*]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/findspark.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    161\u001b[0m             raise Exception(\n\u001b[1;32m    162\u001b[0m                 \"Unable to find py4j in {}, your SPARK_HOME may not be configured correctly\".format(\n\u001b[0;32m--> 163\u001b[0;31m                     \u001b[0mspark_python\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m                 )\n\u001b[1;32m    165\u001b[0m             )\n",
            "\u001b[0;31mException\u001b[0m: Unable to find py4j in /content/spark-2.4.7-bin-hadoop2.7/python, your SPARK_HOME may not be configured correctly"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpuyGonolr5x"
      },
      "source": [
        "spark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFeRjB5XVhTY"
      },
      "source": [
        "# 2.- Importacion de Librerias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kax8lSkCVnFL"
      },
      "source": [
        "# Librerias Spark\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "#Librerias Python\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPcoVuvlVmRj"
      },
      "source": [
        "# 3.- Importacion de Datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsc40jsIVwY0"
      },
      "source": [
        "# Usamos la librería files de colab\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# También se puede importar desde el mismo navegador"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqZ_B3bEmCt7"
      },
      "source": [
        "## 3.1. Importacion desde un csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jm9I-lybmVuZ"
      },
      "source": [
        "Para importar datos desde un csv utilizamos el comando **spark.read.csv**\n",
        "\n",
        "Sintaxis más utilizada:\n",
        "**dfsSpark = spark.read.csv(DATA_PATH + file_name, sep = ',', header=True, inferSchema=True)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_a8O8metmBPv"
      },
      "source": [
        "help(spark.read.csv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoyoNW3eqL9b"
      },
      "source": [
        "Ejercicio 1:\n",
        "Importar el fichero **salario.csv** y ponerle de nombre dfsSalario"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-d-3aToqmNC"
      },
      "source": [
        "# Insertar codigo aqui\n",
        "dfsCSV = spark.read.csv('2015-summary.csv', sep = ',', header=True, inferSchema=True)\n",
        "dfsCSV.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgOEAnGfq6up"
      },
      "source": [
        "## 3.2. Importacion desde un json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEyUes4Jq6ur"
      },
      "source": [
        "Para importar datos desde un csv utilizamos el comando **spark.read.json**\n",
        "\n",
        "Sintaxis más utilizada:\n",
        "**dfsSpark = spark.read.json(DATA_PATH + file_name)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rf1Scryyq6us"
      },
      "source": [
        "help(spark.read.json)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFcX4Hawq6uy"
      },
      "source": [
        "# Importando desde json\n",
        "dfsJson = spark.read.json('2015-summary.json')\n",
        "dfsJson.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_43sW1Fzr_fv"
      },
      "source": [
        "## 3.3. Importacion desde un txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEfcw8ZTr_fx"
      },
      "source": [
        "Para importar datos desde un csv utilizamos el comando **spark.read.text**\n",
        "\n",
        "Sintaxis más utilizada:\n",
        "**dfsSpark = spark.read.text(DATA_PATH + file_name)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dav3zfPBr_fy"
      },
      "source": [
        "help(spark.read.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ej2dZ_wkr_f3"
      },
      "source": [
        "# Importando desde txt\n",
        "dfsTxt = spark.read.text('salario.txt')\n",
        "dfsTxt.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXnXJU3nsj8Z"
      },
      "source": [
        "## 3.4. Importacion desde un parquet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxbn4xdlsj8b"
      },
      "source": [
        "Para importar datos desde un csv utilizamos el comando **spark.read.parquet**\n",
        "\n",
        "Sintaxis más utilizada:\n",
        "**dfsSpark = spark.read.parquet(DATA_PATH + file_name)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jab0_6Mxsj8c"
      },
      "source": [
        "help(spark.read.parquet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyN6Ky0-sj8i"
      },
      "source": [
        "# Importando desde parquet\n",
        "dfsParquet = spark.read.parquet('part-r-00000-1a9822ba-b8fb-4d8e-844a-ea30d0801b9e.gz.parquet')\n",
        "dfsParquet.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gie7H3JFuAT2"
      },
      "source": [
        "## 3.5. Importacion desde un pandas Dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE_rdmS_uAT9"
      },
      "source": [
        "Para importar datos desde un csv utilizamos el comando **spark.createDataFrame()**\n",
        "\n",
        "Sintaxis más utilizada:\n",
        "**dfsSpark = spark.createDataFrame(pandasDataframe)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-Q-VW3MuAT-"
      },
      "source": [
        "help(spark.createDataFrame)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mxlEVs4uAUE"
      },
      "source": [
        "# Importando desde csv\n",
        "dfpMorosidad = pd.read_csv('morosidad.csv')\n",
        "dfpMorosidad.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18E_-3j4u-FH"
      },
      "source": [
        "# Verificando tipo de dato\n",
        "type(dfpMorosidad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIx7vIchvKgL"
      },
      "source": [
        "# Convirtiendo a Spark Dataframe\n",
        "dfsMorosidad = spark.createDataFrame(dfpMorosidad)\n",
        "dfsMorosidad.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJEjP3aEvYqH"
      },
      "source": [
        "# Verificando tipo de dato\n",
        "type(dfsMorosidad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05HR1FDjXlAp"
      },
      "source": [
        "# 4.- Información básica de DataFrames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnjOnHpeXlAq"
      },
      "source": [
        "\n",
        "\n",
        "## 4.1. Previsualización\n",
        "\n",
        "**show** es un método que muestra por pantalla _n_ filas del DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bST3B8a3XlAs"
      },
      "source": [
        "dfsCSV.show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ovMewSLXlAz"
      },
      "source": [
        "\n",
        "\n",
        "## 4.2. Dimensiones\n",
        "\n",
        "En Spark, no existe un método *shape*, por lo que hay que contar por separados las filas y las columnas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxCcILDnXlA0"
      },
      "source": [
        "# Contando el numero de filas\n",
        "dfsCSV.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OfUfptoXlA4"
      },
      "source": [
        "\n",
        "\n",
        "`columns` es un atributo que contiene los nombres de las columnas del DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "h6jfIv3-XlA5"
      },
      "source": [
        "dfsMorosidad.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAMq6fuAXlA-"
      },
      "source": [
        "len(dfsMorosidad.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu11X1KAXlBB"
      },
      "source": [
        "\n",
        "\n",
        "## 4.3. Schema\n",
        "\n",
        "El schema de un dataframe nos muestra como se interpretaran los datos. Esto no significa que los datos estén así. _schema_ es un atributo del objeto, no un método. _printSchema()_ es un método que muestra una versión más entendidible del _schema_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HlSGfQnXlBD"
      },
      "source": [
        "dfsCSV.schema"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HS4KFHPHXlBL"
      },
      "source": [
        "dfsCSV.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEZY2FO8XlBO"
      },
      "source": [
        "\n",
        "\n",
        "## 4.3. dtypes\n",
        "\n",
        "El atributo `dtypes` contiene los nombres de las columnas del dataframe junto con su tipo. Esto permite seleccionar nombres de columnas basados en el tipo, normalmente las variables categóricas (string y boolean) tienen tratamientos distintos a las numéricas (enteras y decimales)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fo7TO5tQXlBO"
      },
      "source": [
        "dfsCSV.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IbXVDXv5lbJ"
      },
      "source": [
        "# 5.- Operaciones con Dataframes de Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jS0OkGny6kky"
      },
      "source": [
        "dfsMorosidad.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2zeHlU8fiAA"
      },
      "source": [
        "## 5.1. Select\n",
        "\n",
        "Filtra las columnas que deseamos mostrar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_kl-qPufiAA"
      },
      "source": [
        "# Forma 1\n",
        "dfsMorosidad.select('meses', 'score', 'zona').show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XC_0lQ_6uUk"
      },
      "source": [
        "# Forma 2\n",
        "dfsMorosidad.select(F.col('meses'), F.col('meses') * 2, F.col('score'), F.col('zona')).show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fr-8GPWgfiAC"
      },
      "source": [
        "# Forma 3\n",
        "columnas = ['meses','score','zona']\n",
        "dfsMorosidad.select(columnas).show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdPJYFuX7ISt"
      },
      "source": [
        "# Combinando las tres formas\n",
        "columnas = ['meses','score','zona']\n",
        "dfsMorosidad.select('ID', F.col('nivel'), *columnas).show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QseBhxTfiAf"
      },
      "source": [
        "## 5.2. Filter - Where\n",
        "\n",
        "filtra resgistros segun cumplan la condicion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSgvCxey8Iaz"
      },
      "source": [
        "dfsParquet.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJmYt80xfiAg"
      },
      "source": [
        "# Forma 1\n",
        "dfsParquet.where(\"DEST_COUNTRY_NAME = 'United States'\").show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RphjZAzqfiAi"
      },
      "source": [
        "# Forma 2\n",
        "dfsParquet.filter(F.col('ORIGIN_COUNTRY_NAME') == 'India').select('ORIGIN_COUNTRY_NAME','count').show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d99lS1tD8uKm"
      },
      "source": [
        "**Ejercicio 1:\n",
        "Genere una consulta que liste los paises cuyo 'count' sea mayor a 25**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LMgFhbPfiAl"
      },
      "source": [
        "# Resolver aqui\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_Ct_whdfiAv"
      },
      "source": [
        "# Varios valores - Forma 1\n",
        "paises = ['United States', 'Egypt', 'Equatorial Guinea']\n",
        "dfsParquet.where(F.col('DEST_COUNTRY_NAME').isin(paises)).show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDI_z46e96B5"
      },
      "source": [
        "# Varios valores - Forma 2\n",
        "dfsParquet.where(\"DEST_COUNTRY_NAME in ('United States', 'Egypt', 'Equatorial Guinea')\").show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhVE4Dbe-Jt2"
      },
      "source": [
        "# Varios valores - Forma 3\n",
        "dfsParquet.where(F.col('DEST_COUNTRY_NAME').isin('United States', 'Egypt', 'Equatorial Guinea')).show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QNeBiEo-Z2o"
      },
      "source": [
        "**OJO**: Para negar podemos usar el operador **~**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "MlEy1HEtfiAw"
      },
      "source": [
        "# Negamos la anterior sentencia\n",
        "dfsParquet.where(~F.col('DEST_COUNTRY_NAME').isin('United States', 'Egypt', 'Equatorial Guinea')).show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOxOor6l_KJk"
      },
      "source": [
        "**Ejercicio 2:\n",
        "Genere una consulta que liste los paises cuyo 'ORIGIN_COUNTRY_NAME' no sea Romania, Ireland ni United States**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJa00xjF_KJm"
      },
      "source": [
        "# Resolver aqui\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixszkgfefiA0"
      },
      "source": [
        "\n",
        "\n",
        "__Combinación de filtros (AND / OR)__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUE4mQsVASzV"
      },
      "source": [
        "__AND__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeNC6fC5fiA0"
      },
      "source": [
        "# Forma 1\n",
        "dfsParquet.where( ( F.col('DEST_COUNTRY_NAME') == 'Malta' ) & \n",
        "                 ( F.col('ORIGIN_COUNTRY_NAME') == 'United States' ) ).show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-kkD3_XfiA2"
      },
      "source": [
        "# Forma 2\n",
        "dfsParquet.where(\"DEST_COUNTRY_NAME = 'Malta' and ORIGIN_COUNTRY_NAME = 'United States'\").show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV0rtyXqAVui"
      },
      "source": [
        "__OR__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3l2-sA6AY8S"
      },
      "source": [
        "# Forma 1\n",
        "dfsParquet.where((F.col('DEST_COUNTRY_NAME') == 'Malta') | (F.col('ORIGIN_COUNTRY_NAME') == 'United States')).show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNWFq0w7AY8W"
      },
      "source": [
        "# Forma 2\n",
        "dfsParquet.where(\"DEST_COUNTRY_NAME = 'Malta' or ORIGIN_COUNTRY_NAME = 'United States'\").show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQuWV0X4A6Lj"
      },
      "source": [
        "**Ejercicio 3:\n",
        "Genere una consulta que liste los paises cuyo 'DEST_COUNTRY_NAME' sea United States y su 'ORIGIN_COUNTRY_NAME' no sea India**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnkShSQmBHuF"
      },
      "source": [
        "# Resolver aqui\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQ3yb4E4fiBV"
      },
      "source": [
        "## 5.3. Group By\n",
        "\n",
        "Resume los registros en gru"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "XfWICXRjfiBV"
      },
      "source": [
        "dfsCSV.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onVemclgfiBX"
      },
      "source": [
        "# Count\n",
        "dfsCSV.where('DEST_COUNTRY_NAME = \"United States\"').groupBy('DEST_COUNTRY_NAME').count().show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWnyeH-yfiBa"
      },
      "source": [
        "# SUM\n",
        "dfsCSV.groupBy('DEST_COUNTRY_NAME').sum('count').show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6Uqva7ufiBc"
      },
      "source": [
        "# Combinando las dos formas\n",
        "# SELECT count(*), SUM(variable)\n",
        "# FROM dfsCSV\n",
        "# GROUP BY DEST_COUNTRY_NAME\n",
        "\n",
        "dfsCSV.groupBy('DEST_COUNTRY_NAME').agg(F.count('*'), F.max('count')).show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2vVelZ8DQZg"
      },
      "source": [
        "**Ejercicio 4:\n",
        "Genere una consulta que agrupe por 'DEST_COUNTRY_NAME' y saque el minimo, maximo y el prmedio de 'count'**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5s7li2wDcF0"
      },
      "source": [
        "# Resuelva aqui\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubebAP_RfiBN"
      },
      "source": [
        "## 5.4. Sort - OrderBy\n",
        "\n",
        "Ordena los registros segun columna."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rLd3PnpENjn"
      },
      "source": [
        "dfsMorosidad.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaEyjhAvfiBO"
      },
      "source": [
        "dfsMorosidad.orderBy(F.col('edad').desc()).show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyaJ1FlDfiBQ"
      },
      "source": [
        "dfsMorosidad.sort(F.col('ingreso').asc()).show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vff-DGYofiBT"
      },
      "source": [
        "dfsMorosidad.sort(F.col('tipo_vivienda').asc(), F.col('ingreso').desc()).show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m40V_psAT_zT"
      },
      "source": [
        "## Funciones Extras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgHgjlEVT_zW"
      },
      "source": [
        "### Clausula When"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "HBZsvts7T_zW"
      },
      "source": [
        "dfsCSV.select('count', F.when(F.col('count') < 25, \"Menores a 25\").otherwise(\"Mayores a 25\")).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vX3f9OsVT_zY"
      },
      "source": [
        "dfsCSV.select('DEST_COUNTRY_NAME', F.when(F.col('count') < 25, \"Menores a 25\").when(F.col('count') < 50, \"Menores a 50\").otherwise(\"Mayores a 100\")).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVwhh_jcT_zZ"
      },
      "source": [
        "### Clausula Like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6c1Wb28T_za"
      },
      "source": [
        "dfsCSV.select(\"DEST_COUNTRY_NAME\", F.col(\"DEST_COUNTRY_NAME\").like(\"%Egy%\")).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V6yo6XgT_zd"
      },
      "source": [
        "### Clausula Startswith - Endswith"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCmPfY9mT_ze"
      },
      "source": [
        "dfsCSV.select(\"DEST_COUNTRY_NAME\", F.col(\"DEST_COUNTRY_NAME\").startswith(\"Uni\")).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "HKW3Y-GAT_zf"
      },
      "source": [
        "dfsCSV.select(\"DEST_COUNTRY_NAME\", F.col(\"DEST_COUNTRY_NAME\").endswith(\"a\")).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gjz_fbXrT_zj"
      },
      "source": [
        "### Clausula Substring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LkQH2WmT_zk"
      },
      "source": [
        "dfsCSV.select(\"DEST_COUNTRY_NAME\", (F.col(\"DEST_COUNTRY_NAME\").substr(2,4)).alias(\"Nombre_Corto\")).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXZj-w96T_zl"
      },
      "source": [
        "### Clausula Between"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfwS_37tT_zl"
      },
      "source": [
        "dfsCSV.select(\"DEST_COUNTRY_NAME\", \"count\", F.col(\"count\").between(25,75)).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19dzyPnrG1bt"
      },
      "source": [
        "dfsCSV.select(\"DEST_COUNTRY_NAME\", \"count\").where(F.col(\"count\").between(25,75)).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05oQP-2xfiBC"
      },
      "source": [
        "### Distinct\n",
        "\n",
        "Una llamada al método `distinct` es lo mismo que al método `dropDuplicates` sin parámetro. Es decir, tiene en cuenta todas las columnas. También se utiliza normalmente para contar los valores únicos de una columna."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70W-Bd--fiBC"
      },
      "source": [
        "# Usamos la funcion distinct\n",
        "dfsCSV.select('DEST_COUNTRY_NAME').distinct().show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-uQlZdXlZHF"
      },
      "source": [
        "### Agregando Columnas\n",
        "\n",
        "Usamos la sentencia **withColumn()**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6ma41GhlxFE"
      },
      "source": [
        "dfsCSV = dfsCSV.withColumn('Conteo', F.col('count') + 5 )\n",
        "dfsCSV.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wou5UCRkT_zm"
      },
      "source": [
        "### Modificando nombres de columnas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1fwbwiPT_zm"
      },
      "source": [
        "dfsCSV = dfsCSV.withColumnRenamed('count', 'cuenta')\n",
        "dfsCSV.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwyKH_Cwm38P"
      },
      "source": [
        "**Ejercicio 5:\n",
        "Genere una nueva columna llamada 'INICIALES' que sean las tres primeras letras del campo 'DEST_COUNTRY_NAME'**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUHH1gOQm38X"
      },
      "source": [
        "# Resuelva aqui\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7D7ACXznf9g"
      },
      "source": [
        "**Ejercicio 6:\n",
        "Genere una nueva columna llamada 'FLAG_CONTEO' que si la variable cuenta es mayor a 25 entonces tome el valor de 1 caso contrario el valor 0**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIHE_n5IngLm"
      },
      "source": [
        "# Resuelva aqui\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmMWPPvrfiAE"
      },
      "source": [
        "\n",
        "\n",
        "### Drop\n",
        "\n",
        "El método `drop` tiene la función contraria al `select`, elimina un subconjunto de columnas. En este caso no se puede pasar una lista de columnas, es necesario utlizar el operador `*` para convertirlo a parámetros indivuales.\n",
        "\n",
        "**OJO:** Si se intenta eliminar una columna que no existe no devuelve error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXNmi6h8fiAF"
      },
      "source": [
        "# Forma 1\n",
        "dfsCSV = dfsCSV.drop('count','conteo')\n",
        "dfsCSV.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwqCiqEDoSVT"
      },
      "source": [
        "# Forma 2\n",
        "columnas = ['DEST_COUNTRY_NAME','ORIGIN_COUNTRY_NAME']\n",
        "dfsCSV = dfsCSV.drop(*columnas)\n",
        "dfsCSV.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}