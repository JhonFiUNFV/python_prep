{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SparkSQL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JhonFiUNFV/python_prep/blob/master/SparkSQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQgCzt-8UrH6"
      },
      "source": [
        "# 1.- Configuracion Entorno Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdKw7_n8UEX9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96d155f8-9d9a-4f73-a552-c8292965e82e"
      },
      "source": [
        "# Descargamos spark con hadoop y Java\n",
        "!apt-get update # Actualizar el gestor de paquetes de Linux\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null # Descargar Java\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-2.4.8/spark-2.4.8-bin-hadoop2.7.tgz # Descargar Apache Spark\n",
        "!tar xf spark-2.4.8-bin-hadoop2.7.tgz # Descomprimir Spark\n",
        "!pip install -q findspark # Para utilizar Spark"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "\r0% [Waiting for headers] [Connected to cloud.r-project.org (65.8.178.6)] [Conne\r0% [1 InRelease gpgv 88.7 kB] [Waiting for headers] [Connected to cloud.r-proje\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\r0% [1 InRelease gpgv 88.7 kB] [Connected to cloud.r-project.org (65.8.178.6)] [\r                                                                               \rHit:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "\r0% [1 InRelease gpgv 88.7 kB] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rHit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "\r0% [1 InRelease gpgv 88.7 kB] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rIgn:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Ign:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:10 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwhBfxJaUq-Z"
      },
      "source": [
        "# Seteamos las variables de entorno\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.8-bin-hadoop2.7\""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgrBPLJeVJzO"
      },
      "source": [
        "# Creamos la conexion a Spark \n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpuyGonolr5x",
        "outputId": "7d4e5d28-4515-4176-f580-8cb41ae8647b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "spark"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fddd5c2f590>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://56b04f571454:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v2.4.8</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFeRjB5XVhTY"
      },
      "source": [
        "# 2.- Importacion de Librerias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kax8lSkCVnFL"
      },
      "source": [
        "# Librerias Spark\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "#Librerias Python\n",
        "import pandas as pd"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPcoVuvlVmRj"
      },
      "source": [
        "# 3.- Importacion de Datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsc40jsIVwY0",
        "outputId": "3c03a35d-f786-4b69-9e32-c8dc77862749",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 39
        }
      },
      "source": [
        "# Usamos la librería files de colab\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# También se puede importar desde el mismo navegador"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a99ee61b-0d90-40ee-8ea9-16c049b04b19\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a99ee61b-0d90-40ee-8ea9-16c049b04b19\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqZ_B3bEmCt7"
      },
      "source": [
        "## 3.1. Importacion desde un csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jm9I-lybmVuZ"
      },
      "source": [
        "Para importar datos desde un csv utilizamos el comando **spark.read.csv**\n",
        "\n",
        "Sintaxis más utilizada:\n",
        "**dfsSpark = spark.read.csv(DATA_PATH + file_name, sep = ',', header=True, inferSchema=True)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_a8O8metmBPv",
        "outputId": "6d6127bf-a488-4710-ef36-3223ae80c236",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "help(spark.read.csv)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on method csv in module pyspark.sql.readwriter:\n",
            "\n",
            "csv(path, schema=None, sep=None, encoding=None, quote=None, escape=None, comment=None, header=None, inferSchema=None, ignoreLeadingWhiteSpace=None, ignoreTrailingWhiteSpace=None, nullValue=None, nanValue=None, positiveInf=None, negativeInf=None, dateFormat=None, timestampFormat=None, maxColumns=None, maxCharsPerColumn=None, maxMalformedLogPerPartition=None, mode=None, columnNameOfCorruptRecord=None, multiLine=None, charToEscapeQuoteEscaping=None, samplingRatio=None, enforceSchema=None, emptyValue=None) method of pyspark.sql.readwriter.DataFrameReader instance\n",
            "    Loads a CSV file and returns the result as a  :class:`DataFrame`.\n",
            "    \n",
            "    This function will go through the input once to determine the input schema if\n",
            "    ``inferSchema`` is enabled. To avoid going through the entire data once, disable\n",
            "    ``inferSchema`` option or specify the schema explicitly using ``schema``.\n",
            "    \n",
            "    :param path: string, or list of strings, for input path(s),\n",
            "                 or RDD of Strings storing CSV rows.\n",
            "    :param schema: an optional :class:`pyspark.sql.types.StructType` for the input schema\n",
            "                   or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
            "    :param sep: sets a single character as a separator for each field and value.\n",
            "                If None is set, it uses the default value, ``,``.\n",
            "    :param encoding: decodes the CSV files by the given encoding type. If None is set,\n",
            "                     it uses the default value, ``UTF-8``.\n",
            "    :param quote: sets a single character used for escaping quoted values where the\n",
            "                  separator can be part of the value. If None is set, it uses the default\n",
            "                  value, ``\"``. If you would like to turn off quotations, you need to set an\n",
            "                  empty string.\n",
            "    :param escape: sets a single character used for escaping quotes inside an already\n",
            "                   quoted value. If None is set, it uses the default value, ``\\``.\n",
            "    :param comment: sets a single character used for skipping lines beginning with this\n",
            "                    character. By default (None), it is disabled.\n",
            "    :param header: uses the first line as names of columns. If None is set, it uses the\n",
            "                   default value, ``false``.\n",
            "                   .. note:: if the given path is a RDD of Strings, this header\n",
            "                   option will remove all lines same with the header if exists.\n",
            "    \n",
            "    :param inferSchema: infers the input schema automatically from data. It requires one extra\n",
            "                   pass over the data. If None is set, it uses the default value, ``false``.\n",
            "    :param enforceSchema: If it is set to ``true``, the specified or inferred schema will be\n",
            "                          forcibly applied to datasource files, and headers in CSV files will be\n",
            "                          ignored. If the option is set to ``false``, the schema will be\n",
            "                          validated against all headers in CSV files or the first header in RDD\n",
            "                          if the ``header`` option is set to ``true``. Field names in the schema\n",
            "                          and column names in CSV headers are checked by their positions\n",
            "                          taking into account ``spark.sql.caseSensitive``. If None is set,\n",
            "                          ``true`` is used by default. Though the default value is ``true``,\n",
            "                          it is recommended to disable the ``enforceSchema`` option\n",
            "                          to avoid incorrect results.\n",
            "    :param ignoreLeadingWhiteSpace: A flag indicating whether or not leading whitespaces from\n",
            "                                    values being read should be skipped. If None is set, it\n",
            "                                    uses the default value, ``false``.\n",
            "    :param ignoreTrailingWhiteSpace: A flag indicating whether or not trailing whitespaces from\n",
            "                                     values being read should be skipped. If None is set, it\n",
            "                                     uses the default value, ``false``.\n",
            "    :param nullValue: sets the string representation of a null value. If None is set, it uses\n",
            "                      the default value, empty string. Since 2.0.1, this ``nullValue`` param\n",
            "                      applies to all supported types including the string type.\n",
            "    :param nanValue: sets the string representation of a non-number value. If None is set, it\n",
            "                     uses the default value, ``NaN``.\n",
            "    :param positiveInf: sets the string representation of a positive infinity value. If None\n",
            "                        is set, it uses the default value, ``Inf``.\n",
            "    :param negativeInf: sets the string representation of a negative infinity value. If None\n",
            "                        is set, it uses the default value, ``Inf``.\n",
            "    :param dateFormat: sets the string that indicates a date format. Custom date formats\n",
            "                       follow the formats at ``java.text.SimpleDateFormat``. This\n",
            "                       applies to date type. If None is set, it uses the\n",
            "                       default value, ``yyyy-MM-dd``.\n",
            "    :param timestampFormat: sets the string that indicates a timestamp format. Custom date\n",
            "                            formats follow the formats at ``java.text.SimpleDateFormat``.\n",
            "                            This applies to timestamp type. If None is set, it uses the\n",
            "                            default value, ``yyyy-MM-dd'T'HH:mm:ss.SSSXXX``.\n",
            "    :param maxColumns: defines a hard limit of how many columns a record can have. If None is\n",
            "                       set, it uses the default value, ``20480``.\n",
            "    :param maxCharsPerColumn: defines the maximum number of characters allowed for any given\n",
            "                              value being read. If None is set, it uses the default value,\n",
            "                              ``-1`` meaning unlimited length.\n",
            "    :param maxMalformedLogPerPartition: this parameter is no longer used since Spark 2.2.0.\n",
            "                                        If specified, it is ignored.\n",
            "    :param mode: allows a mode for dealing with corrupt records during parsing. If None is\n",
            "                 set, it uses the default value, ``PERMISSIVE``. Note that Spark tries to\n",
            "                 parse only required columns in CSV under column pruning. Therefore, corrupt\n",
            "                 records can be different based on required set of fields. This behavior can\n",
            "                 be controlled by ``spark.sql.csv.parser.columnPruning.enabled``\n",
            "                 (enabled by default).\n",
            "    \n",
            "            * ``PERMISSIVE`` : when it meets a corrupted record, puts the malformed string \\\n",
            "              into a field configured by ``columnNameOfCorruptRecord``, and sets other \\\n",
            "              fields to ``null``. To keep corrupt records, an user can set a string type \\\n",
            "              field named ``columnNameOfCorruptRecord`` in an user-defined schema. If a \\\n",
            "              schema does not have the field, it drops corrupt records during parsing. \\\n",
            "              A record with less/more tokens than schema is not a corrupted record to CSV. \\\n",
            "              When it meets a record having fewer tokens than the length of the schema, \\\n",
            "              sets ``null`` to extra fields. When the record has more tokens than the \\\n",
            "              length of the schema, it drops extra tokens.\n",
            "            * ``DROPMALFORMED`` : ignores the whole corrupted records.\n",
            "            * ``FAILFAST`` : throws an exception when it meets corrupted records.\n",
            "    \n",
            "    :param columnNameOfCorruptRecord: allows renaming the new field having malformed string\n",
            "                                      created by ``PERMISSIVE`` mode. This overrides\n",
            "                                      ``spark.sql.columnNameOfCorruptRecord``. If None is set,\n",
            "                                      it uses the value specified in\n",
            "                                      ``spark.sql.columnNameOfCorruptRecord``.\n",
            "    :param multiLine: parse records, which may span multiple lines. If None is\n",
            "                      set, it uses the default value, ``false``.\n",
            "    :param charToEscapeQuoteEscaping: sets a single character used for escaping the escape for\n",
            "                                      the quote character. If None is set, the default value is\n",
            "                                      escape character when escape and quote characters are\n",
            "                                      different, ``\\0`` otherwise.\n",
            "    :param samplingRatio: defines fraction of rows used for schema inferring.\n",
            "                          If None is set, it uses the default value, ``1.0``.\n",
            "    :param emptyValue: sets the string representation of an empty value. If None is set, it uses\n",
            "                       the default value, empty string.\n",
            "    \n",
            "    >>> df = spark.read.csv('python/test_support/sql/ages.csv')\n",
            "    >>> df.dtypes\n",
            "    [('_c0', 'string'), ('_c1', 'string')]\n",
            "    >>> rdd = sc.textFile('python/test_support/sql/ages.csv')\n",
            "    >>> df2 = spark.read.csv(rdd)\n",
            "    >>> df2.dtypes\n",
            "    [('_c0', 'string'), ('_c1', 'string')]\n",
            "    \n",
            "    .. versionadded:: 2.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoyoNW3eqL9b"
      },
      "source": [
        "Ejercicio 1:\n",
        "Importar el fichero **salario.csv** y ponerle de nombre dfsSalario"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-d-3aToqmNC",
        "outputId": "43616070-5f81-4adb-f25f-94c3f7488eec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Insertar codigo aqui\n",
        "dfsCSV = spark.read.csv('2015-summary.csv', sep = ',', header=True, inferSchema=True)\n",
        "dfsCSV.show(5)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|    United States|            Romania|   15|\n",
            "|    United States|            Croatia|    1|\n",
            "|    United States|            Ireland|  344|\n",
            "|            Egypt|      United States|   15|\n",
            "|    United States|              India|   62|\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgOEAnGfq6up"
      },
      "source": [
        "## 3.2. Importacion desde un json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEyUes4Jq6ur"
      },
      "source": [
        "Para importar datos desde un csv utilizamos el comando **spark.read.json**\n",
        "\n",
        "Sintaxis más utilizada:\n",
        "**dfsSpark = spark.read.json(DATA_PATH + file_name)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rf1Scryyq6us",
        "outputId": "bf8bb322-df18-4f4e-ed57-704479a4d743",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "help(spark.read.json)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on method json in module pyspark.sql.readwriter:\n",
            "\n",
            "json(path, schema=None, primitivesAsString=None, prefersDecimal=None, allowComments=None, allowUnquotedFieldNames=None, allowSingleQuotes=None, allowNumericLeadingZero=None, allowBackslashEscapingAnyCharacter=None, mode=None, columnNameOfCorruptRecord=None, dateFormat=None, timestampFormat=None, multiLine=None, allowUnquotedControlChars=None, lineSep=None, samplingRatio=None, dropFieldIfAllNull=None, encoding=None) method of pyspark.sql.readwriter.DataFrameReader instance\n",
            "    Loads JSON files and returns the results as a :class:`DataFrame`.\n",
            "    \n",
            "    `JSON Lines <http://jsonlines.org/>`_ (newline-delimited JSON) is supported by default.\n",
            "    For JSON (one record per file), set the ``multiLine`` parameter to ``true``.\n",
            "    \n",
            "    If the ``schema`` parameter is not specified, this function goes\n",
            "    through the input once to determine the input schema.\n",
            "    \n",
            "    :param path: string represents path to the JSON dataset, or a list of paths,\n",
            "                 or RDD of Strings storing JSON objects.\n",
            "    :param schema: an optional :class:`pyspark.sql.types.StructType` for the input schema or\n",
            "                   a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
            "    :param primitivesAsString: infers all primitive values as a string type. If None is set,\n",
            "                               it uses the default value, ``false``.\n",
            "    :param prefersDecimal: infers all floating-point values as a decimal type. If the values\n",
            "                           do not fit in decimal, then it infers them as doubles. If None is\n",
            "                           set, it uses the default value, ``false``.\n",
            "    :param allowComments: ignores Java/C++ style comment in JSON records. If None is set,\n",
            "                          it uses the default value, ``false``.\n",
            "    :param allowUnquotedFieldNames: allows unquoted JSON field names. If None is set,\n",
            "                                    it uses the default value, ``false``.\n",
            "    :param allowSingleQuotes: allows single quotes in addition to double quotes. If None is\n",
            "                                    set, it uses the default value, ``true``.\n",
            "    :param allowNumericLeadingZero: allows leading zeros in numbers (e.g. 00012). If None is\n",
            "                                    set, it uses the default value, ``false``.\n",
            "    :param allowBackslashEscapingAnyCharacter: allows accepting quoting of all character\n",
            "                                               using backslash quoting mechanism. If None is\n",
            "                                               set, it uses the default value, ``false``.\n",
            "    :param mode: allows a mode for dealing with corrupt records during parsing. If None is\n",
            "                 set, it uses the default value, ``PERMISSIVE``.\n",
            "    \n",
            "            * ``PERMISSIVE`` : when it meets a corrupted record, puts the malformed string                   into a field configured by ``columnNameOfCorruptRecord``, and sets other                   fields to ``null``. To keep corrupt records, an user can set a string type                   field named ``columnNameOfCorruptRecord`` in an user-defined schema. If a                   schema does not have the field, it drops corrupt records during parsing.                   When inferring a schema, it implicitly adds a ``columnNameOfCorruptRecord``                   field in an output schema.\n",
            "            *  ``DROPMALFORMED`` : ignores the whole corrupted records.\n",
            "            *  ``FAILFAST`` : throws an exception when it meets corrupted records.\n",
            "    \n",
            "    :param columnNameOfCorruptRecord: allows renaming the new field having malformed string\n",
            "                                      created by ``PERMISSIVE`` mode. This overrides\n",
            "                                      ``spark.sql.columnNameOfCorruptRecord``. If None is set,\n",
            "                                      it uses the value specified in\n",
            "                                      ``spark.sql.columnNameOfCorruptRecord``.\n",
            "    :param dateFormat: sets the string that indicates a date format. Custom date formats\n",
            "                       follow the formats at ``java.text.SimpleDateFormat``. This\n",
            "                       applies to date type. If None is set, it uses the\n",
            "                       default value, ``yyyy-MM-dd``.\n",
            "    :param timestampFormat: sets the string that indicates a timestamp format. Custom date\n",
            "                            formats follow the formats at ``java.text.SimpleDateFormat``.\n",
            "                            This applies to timestamp type. If None is set, it uses the\n",
            "                            default value, ``yyyy-MM-dd'T'HH:mm:ss.SSSXXX``.\n",
            "    :param multiLine: parse one record, which may span multiple lines, per file. If None is\n",
            "                      set, it uses the default value, ``false``.\n",
            "    :param allowUnquotedControlChars: allows JSON Strings to contain unquoted control\n",
            "                                      characters (ASCII characters with value less than 32,\n",
            "                                      including tab and line feed characters) or not.\n",
            "    :param encoding: allows to forcibly set one of standard basic or extended encoding for\n",
            "                     the JSON files. For example UTF-16BE, UTF-32LE. If None is set,\n",
            "                     the encoding of input JSON will be detected automatically\n",
            "                     when the multiLine option is set to ``true``.\n",
            "    :param lineSep: defines the line separator that should be used for parsing. If None is\n",
            "                    set, it covers all ``\\r``, ``\\r\\n`` and ``\\n``.\n",
            "    :param samplingRatio: defines fraction of input JSON objects used for schema inferring.\n",
            "                          If None is set, it uses the default value, ``1.0``.\n",
            "    :param dropFieldIfAllNull: whether to ignore column of all null values or empty\n",
            "                               array/struct during schema inference. If None is set, it\n",
            "                               uses the default value, ``false``.\n",
            "    \n",
            "    >>> df1 = spark.read.json('python/test_support/sql/people.json')\n",
            "    >>> df1.dtypes\n",
            "    [('age', 'bigint'), ('name', 'string')]\n",
            "    >>> rdd = sc.textFile('python/test_support/sql/people.json')\n",
            "    >>> df2 = spark.read.json(rdd)\n",
            "    >>> df2.dtypes\n",
            "    [('age', 'bigint'), ('name', 'string')]\n",
            "    \n",
            "    .. versionadded:: 1.4\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFcX4Hawq6uy",
        "outputId": "b1733e46-4f17-4b71-8c11-49056a1165fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Importando desde json\n",
        "dfsJson = spark.read.json('2015-summary.json')\n",
        "dfsJson.show(5)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|    United States|            Romania|   15|\n",
            "|    United States|            Croatia|    1|\n",
            "|    United States|            Ireland|  344|\n",
            "|            Egypt|      United States|   15|\n",
            "|    United States|              India|   62|\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_43sW1Fzr_fv"
      },
      "source": [
        "## 3.3. Importacion desde un txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEfcw8ZTr_fx"
      },
      "source": [
        "Para importar datos desde un csv utilizamos el comando **spark.read.text**\n",
        "\n",
        "Sintaxis más utilizada:\n",
        "**dfsSpark = spark.read.text(DATA_PATH + file_name)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dav3zfPBr_fy",
        "outputId": "15a0e811-a66d-415e-ef6a-25dc2df7ce04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "help(spark.read.text)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on method text in module pyspark.sql.readwriter:\n",
            "\n",
            "text(paths, wholetext=False, lineSep=None) method of pyspark.sql.readwriter.DataFrameReader instance\n",
            "    Loads text files and returns a :class:`DataFrame` whose schema starts with a\n",
            "    string column named \"value\", and followed by partitioned columns if there\n",
            "    are any.\n",
            "    \n",
            "    By default, each line in the text file is a new row in the resulting DataFrame.\n",
            "    \n",
            "    :param paths: string, or list of strings, for input path(s).\n",
            "    :param wholetext: if true, read each file from input path(s) as a single row.\n",
            "    :param lineSep: defines the line separator that should be used for parsing. If None is\n",
            "                    set, it covers all ``\\r``, ``\\r\\n`` and ``\\n``.\n",
            "    \n",
            "    >>> df = spark.read.text('python/test_support/sql/text-test.txt')\n",
            "    >>> df.collect()\n",
            "    [Row(value='hello'), Row(value='this')]\n",
            "    >>> df = spark.read.text('python/test_support/sql/text-test.txt', wholetext=True)\n",
            "    >>> df.collect()\n",
            "    [Row(value='hello\\nthis')]\n",
            "    \n",
            "    .. versionadded:: 1.6\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ej2dZ_wkr_f3",
        "outputId": "94adb9a7-91ae-4558-b2e3-d0a14cf254a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Importando desde txt\n",
        "dfsTxt = spark.read.text('salario.txt')\n",
        "dfsTxt.show(5)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|               value|\n",
            "+--------------------+\n",
            "|rank,discipline,y...|\n",
            "|Prof,B,19,18,Male...|\n",
            "|Prof,B,20,16,Male...|\n",
            "|AsstProf,B,4,3,Ma...|\n",
            "|Prof,B,45,39,Male...|\n",
            "+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfsTxt = spark.read.csv('salario.txt', header=True)\n",
        "dfsTxt.show(5)"
      ],
      "metadata": {
        "id": "GMu9qctfRXd7",
        "outputId": "6326d399-108e-42ba-f5a9-776afa4e8744",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+-------------+-----------+----------+------+\n",
            "|    rank|discipline|yrs_since_phd|yrs_service|       sex|salary|\n",
            "+--------+----------+-------------+-----------+----------+------+\n",
            "|    Prof|         B|           19|         18|Male      |139750|\n",
            "|    Prof|         B|           20|         16|Male      |173200|\n",
            "|AsstProf|         B|            4|          3|Male      | 79750|\n",
            "|    Prof|         B|           45|         39|Male      |115000|\n",
            "|    Prof|         B|           40|         41|Male      |141500|\n",
            "+--------+----------+-------------+-----------+----------+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXnXJU3nsj8Z"
      },
      "source": [
        "## 3.4. Importacion desde un parquet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxbn4xdlsj8b"
      },
      "source": [
        "Para importar datos desde un csv utilizamos el comando **spark.read.parquet**\n",
        "\n",
        "Sintaxis más utilizada:\n",
        "**dfsSpark = spark.read.parquet(DATA_PATH + file_name)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jab0_6Mxsj8c",
        "outputId": "f8228ad0-1e9d-415e-de9b-9bf41189ba96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "help(spark.read.parquet)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on method parquet in module pyspark.sql.readwriter:\n",
            "\n",
            "parquet(*paths) method of pyspark.sql.readwriter.DataFrameReader instance\n",
            "    Loads Parquet files, returning the result as a :class:`DataFrame`.\n",
            "    \n",
            "    You can set the following Parquet-specific option(s) for reading Parquet files:\n",
            "        * ``mergeSchema``: sets whether we should merge schemas collected from all                 Parquet part-files. This will override ``spark.sql.parquet.mergeSchema``.                 The default value is specified in ``spark.sql.parquet.mergeSchema``.\n",
            "    \n",
            "    >>> df = spark.read.parquet('python/test_support/sql/parquet_partitioned')\n",
            "    >>> df.dtypes\n",
            "    [('name', 'string'), ('year', 'int'), ('month', 'int'), ('day', 'int')]\n",
            "    \n",
            "    .. versionadded:: 1.4\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyN6Ky0-sj8i",
        "outputId": "05b281c8-07c3-45eb-fd53-3e13b0bb5e23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Importando desde parquet\n",
        "dfsParquet = spark.read.parquet('part-r-00000-1a9822ba-b8fb-4d8e-844a-ea30d0801b9e.gz.parquet')\n",
        "dfsParquet.show(5)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|    United States|            Romania|    1|\n",
            "|    United States|            Ireland|  264|\n",
            "|    United States|              India|   69|\n",
            "|            Egypt|      United States|   24|\n",
            "|Equatorial Guinea|      United States|    1|\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gie7H3JFuAT2"
      },
      "source": [
        "## 3.5. Importacion desde un pandas Dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE_rdmS_uAT9"
      },
      "source": [
        "Para importar datos desde un csv utilizamos el comando **spark.createDataFrame()**\n",
        "\n",
        "Sintaxis más utilizada:\n",
        "**dfsSpark = spark.createDataFrame(pandasDataframe)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-Q-VW3MuAT-",
        "outputId": "811f1d0d-bd17-443e-baae-33494473b2e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "help(spark.createDataFrame)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on method createDataFrame in module pyspark.sql.session:\n",
            "\n",
            "createDataFrame(data, schema=None, samplingRatio=None, verifySchema=True) method of pyspark.sql.session.SparkSession instance\n",
            "    Creates a :class:`DataFrame` from an :class:`RDD`, a list or a :class:`pandas.DataFrame`.\n",
            "    \n",
            "    When ``schema`` is a list of column names, the type of each column\n",
            "    will be inferred from ``data``.\n",
            "    \n",
            "    When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n",
            "    from ``data``, which should be an RDD of either :class:`Row`,\n",
            "    :class:`namedtuple`, or :class:`dict`.\n",
            "    \n",
            "    When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must match\n",
            "    the real data, or an exception will be thrown at runtime. If the given schema is not\n",
            "    :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n",
            "    :class:`pyspark.sql.types.StructType` as its only field, and the field name will be \"value\".\n",
            "    Each record will also be wrapped into a tuple, which can be converted to row later.\n",
            "    \n",
            "    If schema inference is needed, ``samplingRatio`` is used to determined the ratio of\n",
            "    rows used for schema inference. The first row will be used if ``samplingRatio`` is ``None``.\n",
            "    \n",
            "    :param data: an RDD of any kind of SQL data representation (e.g. row, tuple, int, boolean,\n",
            "        etc.), :class:`list`, or :class:`pandas.DataFrame`.\n",
            "    :param schema: a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n",
            "        column names, default is ``None``.  The data type string format equals to\n",
            "        :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n",
            "        omit the ``struct<>`` and atomic types use ``typeName()`` as their format, e.g. use\n",
            "        ``byte`` instead of ``tinyint`` for :class:`pyspark.sql.types.ByteType`. We can also use\n",
            "        ``int`` as a short name for ``IntegerType``.\n",
            "    :param samplingRatio: the sample ratio of rows used for inferring\n",
            "    :param verifySchema: verify data types of every row against schema.\n",
            "    :return: :class:`DataFrame`\n",
            "    \n",
            "    .. versionchanged:: 2.1\n",
            "       Added verifySchema.\n",
            "    \n",
            "    .. note:: Usage with spark.sql.execution.arrow.enabled=True is experimental.\n",
            "    \n",
            "    >>> l = [('Alice', 1)]\n",
            "    >>> spark.createDataFrame(l).collect()\n",
            "    [Row(_1='Alice', _2=1)]\n",
            "    >>> spark.createDataFrame(l, ['name', 'age']).collect()\n",
            "    [Row(name='Alice', age=1)]\n",
            "    \n",
            "    >>> d = [{'name': 'Alice', 'age': 1}]\n",
            "    >>> spark.createDataFrame(d).collect()\n",
            "    [Row(age=1, name='Alice')]\n",
            "    \n",
            "    >>> rdd = sc.parallelize(l)\n",
            "    >>> spark.createDataFrame(rdd).collect()\n",
            "    [Row(_1='Alice', _2=1)]\n",
            "    >>> df = spark.createDataFrame(rdd, ['name', 'age'])\n",
            "    >>> df.collect()\n",
            "    [Row(name='Alice', age=1)]\n",
            "    \n",
            "    >>> from pyspark.sql import Row\n",
            "    >>> Person = Row('name', 'age')\n",
            "    >>> person = rdd.map(lambda r: Person(*r))\n",
            "    >>> df2 = spark.createDataFrame(person)\n",
            "    >>> df2.collect()\n",
            "    [Row(name='Alice', age=1)]\n",
            "    \n",
            "    >>> from pyspark.sql.types import *\n",
            "    >>> schema = StructType([\n",
            "    ...    StructField(\"name\", StringType(), True),\n",
            "    ...    StructField(\"age\", IntegerType(), True)])\n",
            "    >>> df3 = spark.createDataFrame(rdd, schema)\n",
            "    >>> df3.collect()\n",
            "    [Row(name='Alice', age=1)]\n",
            "    \n",
            "    >>> spark.createDataFrame(df.toPandas()).collect()  # doctest: +SKIP\n",
            "    [Row(name='Alice', age=1)]\n",
            "    >>> spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n",
            "    [Row(0=1, 1=2)]\n",
            "    \n",
            "    >>> spark.createDataFrame(rdd, \"a: string, b: int\").collect()\n",
            "    [Row(a='Alice', b=1)]\n",
            "    >>> rdd = rdd.map(lambda row: row[1])\n",
            "    >>> spark.createDataFrame(rdd, \"int\").collect()\n",
            "    [Row(value=1)]\n",
            "    >>> spark.createDataFrame(rdd, \"boolean\").collect() # doctest: +IGNORE_EXCEPTION_DETAIL\n",
            "    Traceback (most recent call last):\n",
            "        ...\n",
            "    Py4JJavaError: ...\n",
            "    \n",
            "    .. versionadded:: 2.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mxlEVs4uAUE",
        "outputId": "f4f592ff-ee62-4385-a447-5f188bdd52ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "# Importando desde csv\n",
        "dfpMorosidad = pd.read_csv('morosidad.csv')\n",
        "dfpMorosidad.head(5)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   ID  edad  meses  max_ant   ingreso  score  tipo_vivienda  zona  nivel\n",
              "0   1    28    122        9   1115.09    206              4     1      5\n",
              "1   2    27    173       61   2450.00    205              2     1      5\n",
              "2   3    37    185        4   1390.00    177              2     3      5\n",
              "3   4    34    262       20   2200.00    216              2     1      2\n",
              "4   5    40    150       17  10000.00    185              2     1      2"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b8a2e165-16c8-4cd7-8a6b-221d5c4bfe54\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>edad</th>\n",
              "      <th>meses</th>\n",
              "      <th>max_ant</th>\n",
              "      <th>ingreso</th>\n",
              "      <th>score</th>\n",
              "      <th>tipo_vivienda</th>\n",
              "      <th>zona</th>\n",
              "      <th>nivel</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>28</td>\n",
              "      <td>122</td>\n",
              "      <td>9</td>\n",
              "      <td>1115.09</td>\n",
              "      <td>206</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>27</td>\n",
              "      <td>173</td>\n",
              "      <td>61</td>\n",
              "      <td>2450.00</td>\n",
              "      <td>205</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>37</td>\n",
              "      <td>185</td>\n",
              "      <td>4</td>\n",
              "      <td>1390.00</td>\n",
              "      <td>177</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>34</td>\n",
              "      <td>262</td>\n",
              "      <td>20</td>\n",
              "      <td>2200.00</td>\n",
              "      <td>216</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>40</td>\n",
              "      <td>150</td>\n",
              "      <td>17</td>\n",
              "      <td>10000.00</td>\n",
              "      <td>185</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b8a2e165-16c8-4cd7-8a6b-221d5c4bfe54')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b8a2e165-16c8-4cd7-8a6b-221d5c4bfe54 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b8a2e165-16c8-4cd7-8a6b-221d5c4bfe54');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18E_-3j4u-FH",
        "outputId": "8a01292c-9d2b-4618-eee3-2c861d899b0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Verificando tipo de dato\n",
        "type(dfpMorosidad)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIx7vIchvKgL",
        "outputId": "824640b5-b36f-413a-8b52-9a06a8a92069",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Convirtiendo a Spark Dataframe\n",
        "dfsMorosidad = spark.createDataFrame(dfpMorosidad)\n",
        "dfsMorosidad.show(5)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+-----+-------+-------+-----+-------------+----+-----+\n",
            "| ID|edad|meses|max_ant|ingreso|score|tipo_vivienda|zona|nivel|\n",
            "+---+----+-----+-------+-------+-----+-------------+----+-----+\n",
            "|  1|  28|  122|      9|1115.09|  206|            4|   1|    5|\n",
            "|  2|  27|  173|     61| 2450.0|  205|            2|   1|    5|\n",
            "|  3|  37|  185|      4| 1390.0|  177|            2|   3|    5|\n",
            "|  4|  34|  262|     20| 2200.0|  216|            2|   1|    2|\n",
            "|  5|  40|  150|     17|10000.0|  185|            2|   1|    2|\n",
            "+---+----+-----+-------+-------+-----+-------------+----+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJEjP3aEvYqH",
        "outputId": "47eb4f08-dbf3-4b1d-e84e-65816a3d7ace",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Verificando tipo de dato\n",
        "type(dfsMorosidad)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.dataframe.DataFrame"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05HR1FDjXlAp"
      },
      "source": [
        "# 4.- Información básica de DataFrames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnjOnHpeXlAq"
      },
      "source": [
        "\n",
        "\n",
        "## 4.1. Previsualización\n",
        "\n",
        "**show** es un método que muestra por pantalla _n_ filas del DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bST3B8a3XlAs",
        "outputId": "ba209271-5f2c-4968-b0cf-116c37b82f64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dfsCSV.show(10)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|    United States|            Romania|   15|\n",
            "|    United States|            Croatia|    1|\n",
            "|    United States|            Ireland|  344|\n",
            "|            Egypt|      United States|   15|\n",
            "|    United States|              India|   62|\n",
            "|    United States|          Singapore|    1|\n",
            "|    United States|            Grenada|   62|\n",
            "|       Costa Rica|      United States|  588|\n",
            "|          Senegal|      United States|   40|\n",
            "|          Moldova|      United States|    1|\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ovMewSLXlAz"
      },
      "source": [
        "\n",
        "\n",
        "## 4.2. Dimensiones\n",
        "\n",
        "En Spark, no existe un método *shape*, por lo que hay que contar por separados las filas y las columnas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxCcILDnXlA0",
        "outputId": "b2128445-5c0d-4632-af5b-1c2ff2ebe782",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Contando el numero de filas\n",
        "dfsCSV.count()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "256"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OfUfptoXlA4"
      },
      "source": [
        "\n",
        "\n",
        "`columns` es un atributo que contiene los nombres de las columnas del DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "h6jfIv3-XlA5",
        "outputId": "b027d712-bf23-427d-94d4-d5c383848dab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dfsMorosidad.columns"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ID',\n",
              " 'edad',\n",
              " 'meses',\n",
              " 'max_ant',\n",
              " 'ingreso',\n",
              " 'score',\n",
              " 'tipo_vivienda',\n",
              " 'zona',\n",
              " 'nivel']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAMq6fuAXlA-",
        "outputId": "7a9490e5-bb7f-446d-c165-924cb28332f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(dfsMorosidad.columns)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"La cantidad de filas es: {dfsCSV.count()} y la cantidad de columna es {len(dfsMorosidad.columns)}\") "
      ],
      "metadata": {
        "id": "_wgxuJ7UXydj",
        "outputId": "93781328-e35e-4299-e48e-cdce0dba718a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La cantidad de filas es: 256 y la cantidad de columna es 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu11X1KAXlBB"
      },
      "source": [
        "\n",
        "\n",
        "## 4.3. Schema\n",
        "\n",
        "El schema de un dataframe nos muestra como se interpretaran los datos. Esto no significa que los datos estén así. _schema_ es un atributo del objeto, no un método. _printSchema()_ es un método que muestra una versión más entendidible del _schema_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HlSGfQnXlBD",
        "outputId": "f4e3049d-e657-4314-9e05-1c2d25f24bd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dfsCSV.schema"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true),StructField(ORIGIN_COUNTRY_NAME,StringType,true),StructField(count,IntegerType,true)))"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HS4KFHPHXlBL",
        "outputId": "fbb3db20-7037-42b1-8c50-1f07e7f7cf40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dfsCSV.printSchema()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
            " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
            " |-- count: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEZY2FO8XlBO"
      },
      "source": [
        "\n",
        "\n",
        "## 4.3. dtypes\n",
        "\n",
        "El atributo `dtypes` contiene los nombres de las columnas del dataframe junto con su tipo. Esto permite seleccionar nombres de columnas basados en el tipo, normalmente las variables categóricas (string y boolean) tienen tratamientos distintos a las numéricas (enteras y decimales)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fo7TO5tQXlBO",
        "outputId": "245569f6-0b6f-47c5-9cfd-6bdc5b02f725",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dfsCSV.dtypes"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('DEST_COUNTRY_NAME', 'string'),\n",
              " ('ORIGIN_COUNTRY_NAME', 'string'),\n",
              " ('count', 'int')]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for columna, tipoDato in dfsCSV.dtypes:\n",
        "  if tipoDato == 'string':\n",
        "    print(columna, tipoDato)"
      ],
      "metadata": {
        "id": "fnMjG-VOYvF3",
        "outputId": "8616cd97-9465-42b5-ceda-fbdc3ebb9e90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEST_COUNTRY_NAME string\n",
            "ORIGIN_COUNTRY_NAME string\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Insertar codigo aqui\n",
        "dfsCSV = spark.read.csv('2015-summary.csv', sep = ',', header = True)\n",
        "dfsCSV.show(5)"
      ],
      "metadata": {
        "id": "q3HmnCzSZY4T",
        "outputId": "fdb02d71-91fb-446b-a209-bc93d2057c98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|    United States|            Romania|   15|\n",
            "|    United States|            Croatia|    1|\n",
            "|    United States|            Ireland|  344|\n",
            "|            Egypt|      United States|   15|\n",
            "|    United States|              India|   62|\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfsCSV.dtypes"
      ],
      "metadata": {
        "id": "aeWbZRtVZaBa",
        "outputId": "14728680-74b5-46be-a042-9a3f3e0dd869",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('DEST_COUNTRY_NAME', 'string'),\n",
              " ('ORIGIN_COUNTRY_NAME', 'string'),\n",
              " ('count', 'string')]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IbXVDXv5lbJ"
      },
      "source": [
        "# 5.- Operaciones con Dataframes de Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jS0OkGny6kky"
      },
      "source": [
        "dfsMorosidad.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2zeHlU8fiAA"
      },
      "source": [
        "## 5.1. Select\n",
        "\n",
        "Filtra las columnas que deseamos mostrar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_kl-qPufiAA",
        "outputId": "b6ae9fa9-b573-42fb-8150-7338f7ac970a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Forma 1\n",
        "dfsMorosidad.select('meses', 'score', 'zona').show(5)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+----+\n",
            "|meses|score|zona|\n",
            "+-----+-----+----+\n",
            "|  122|  206|   1|\n",
            "|  173|  205|   1|\n",
            "|  185|  177|   3|\n",
            "|  262|  216|   1|\n",
            "|  150|  185|   1|\n",
            "+-----+-----+----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XC_0lQ_6uUk",
        "outputId": "27eb0825-0611-462b-caf1-33dba2a1d15a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Forma 2\n",
        "dfsMorosidad.select(F.col('meses'), F.col('meses') * 2, F.col('score'), F.col('zona')).show(5)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----------+-----+----+\n",
            "|meses|(meses * 2)|score|zona|\n",
            "+-----+-----------+-----+----+\n",
            "|  122|        244|  206|   1|\n",
            "|  173|        346|  205|   1|\n",
            "|  185|        370|  177|   3|\n",
            "|  262|        524|  216|   1|\n",
            "|  150|        300|  185|   1|\n",
            "+-----+-----------+-----+----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fr-8GPWgfiAC",
        "outputId": "c14381d5-1925-40dc-fd26-1c105f243c20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Forma 3\n",
        "columnas = ['meses','score','zona']\n",
        "dfsMorosidad.select(columnas).show(5)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+----+\n",
            "|meses|score|zona|\n",
            "+-----+-----+----+\n",
            "|  122|  206|   1|\n",
            "|  173|  205|   1|\n",
            "|  185|  177|   3|\n",
            "|  262|  216|   1|\n",
            "|  150|  185|   1|\n",
            "+-----+-----+----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdPJYFuX7ISt",
        "outputId": "04525530-8acb-4ca8-87ee-53a1d4ebc2c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Combinando las tres formas\n",
        "columnas = ['meses',F.col('score')+ 2,'zona']\n",
        "dfsMorosidad.select('ID', F.col('nivel') + 1, *columnas).show(5)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----------+-----+-----------+----+\n",
            "| ID|(nivel + 1)|meses|(score + 2)|zona|\n",
            "+---+-----------+-----+-----------+----+\n",
            "|  1|          6|  122|        208|   1|\n",
            "|  2|          6|  173|        207|   1|\n",
            "|  3|          6|  185|        179|   3|\n",
            "|  4|          3|  262|        218|   1|\n",
            "|  5|          3|  150|        187|   1|\n",
            "+---+-----------+-----+-----------+----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QseBhxTfiAf"
      },
      "source": [
        "## 5.2. Filter - Where\n",
        "\n",
        "filtra resgistros segun cumplan la condicion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSgvCxey8Iaz",
        "outputId": "11f203b6-3c53-4b4b-e0f0-20133b82c444",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dfsParquet.show(5)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|    United States|            Romania|    1|\n",
            "|    United States|            Ireland|  264|\n",
            "|    United States|              India|   69|\n",
            "|            Egypt|      United States|   24|\n",
            "|Equatorial Guinea|      United States|    1|\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJmYt80xfiAg",
        "outputId": "b5d9561c-2b34-4611-a944-01955b2c53ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Forma 1\n",
        "dfsParquet.where(\"DEST_COUNTRY_NAME = 'United States'\").show(5)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|    United States|            Romania|    1|\n",
            "|    United States|            Ireland|  264|\n",
            "|    United States|              India|   69|\n",
            "|    United States|          Singapore|   25|\n",
            "|    United States|            Grenada|   54|\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RphjZAzqfiAi",
        "outputId": "e0f013ed-bee8-40d5-a812-d404d9440a92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Forma 2\n",
        "dfsParquet.filter(F.col('ORIGIN_COUNTRY_NAME') == 'India').select('ORIGIN_COUNTRY_NAME','count').show(5)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-----+\n",
            "|ORIGIN_COUNTRY_NAME|count|\n",
            "+-------------------+-----+\n",
            "|              India|   69|\n",
            "+-------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d99lS1tD8uKm"
      },
      "source": [
        "**Ejercicio 1:\n",
        "Genere una consulta que liste los paises cuyo 'count' sea mayor a 25**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LMgFhbPfiAl",
        "outputId": "3b166b9c-8da4-4cd2-c769-1da452f1dab7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Resolver aqui\n",
        "dfsParquet.filter(F.col('count')>25).select('DEST_COUNTRY_NAME','count').show(5)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-----+\n",
            "|DEST_COUNTRY_NAME|count|\n",
            "+-----------------+-----+\n",
            "|    United States|  264|\n",
            "|    United States|   69|\n",
            "|    United States|   54|\n",
            "|       Costa Rica|  477|\n",
            "|          Senegal|   29|\n",
            "+-----------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Forma 1\n",
        "dfsParquet.where(\"count>25\").show(5)"
      ],
      "metadata": {
        "id": "b-DBscpkfvyR",
        "outputId": "93656c4c-5773-4c45-a3bf-10c434285cb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|    United States|            Ireland|  264|\n",
            "|    United States|              India|   69|\n",
            "|    United States|            Grenada|   54|\n",
            "|       Costa Rica|      United States|  477|\n",
            "|          Senegal|      United States|   29|\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_Ct_whdfiAv",
        "outputId": "dfdca6ea-2040-4853-d6f6-6ab7cd3fe20c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Varios valores - Forma 1\n",
        "paises = ['United States', 'Egypt', 'Equatorial Guinea']\n",
        "dfsParquet.where(F.col('DEST_COUNTRY_NAME').isin(paises)).show(5)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|    United States|            Romania|    1|\n",
            "|    United States|            Ireland|  264|\n",
            "|    United States|              India|   69|\n",
            "|            Egypt|      United States|   24|\n",
            "|Equatorial Guinea|      United States|    1|\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SELECT *\n",
        "# FROM dfsParquet\n",
        "# WHERE DEST_COUNTRY_NAME in ('United States', 'Egypt', 'Equatorial Guinea')"
      ],
      "metadata": {
        "id": "yT0uW1hegi0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDI_z46e96B5",
        "outputId": "117b5cb9-a4cf-4319-d413-d3eb2c75498c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Varios valores - Forma 2\n",
        "dfsParquet.where(\"DEST_COUNTRY_NAME in ('United States', 'Egypt', 'Equatorial Guinea')\").show(5)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|    United States|            Romania|    1|\n",
            "|    United States|            Ireland|  264|\n",
            "|    United States|              India|   69|\n",
            "|            Egypt|      United States|   24|\n",
            "|Equatorial Guinea|      United States|    1|\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhVE4Dbe-Jt2",
        "outputId": "dde28e47-a7e6-4888-d300-004faf27393f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Varios valores - Forma 3\n",
        "dfsParquet.where(F.col('DEST_COUNTRY_NAME').isin('United States', 'Egypt', 'Equatorial Guinea')).show(5)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|    United States|            Romania|    1|\n",
            "|    United States|            Ireland|  264|\n",
            "|    United States|              India|   69|\n",
            "|            Egypt|      United States|   24|\n",
            "|Equatorial Guinea|      United States|    1|\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QNeBiEo-Z2o"
      },
      "source": [
        "**OJO**: Para negar podemos usar el operador **~**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SELECT *\n",
        "# FROM dfsParquet\n",
        "# WHERE DEST_COUNTRY_NAME not in ('United States', 'Egypt', 'Equatorial Guinea')"
      ],
      "metadata": {
        "id": "g9m7btfkhTSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "MlEy1HEtfiAw",
        "outputId": "09ea8e7d-ffe0-4f96-b40d-890d61c8d484",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Negamos la anterior sentencia\n",
        "dfsParquet.where(~F.col('DEST_COUNTRY_NAME').isin('United States', 'Egypt', 'Equatorial Guinea')).show(5)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|       Costa Rica|      United States|  477|\n",
            "|          Senegal|      United States|   29|\n",
            "|           Guyana|      United States|   17|\n",
            "|            Malta|      United States|    1|\n",
            "|          Bolivia|      United States|   46|\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOxOor6l_KJk"
      },
      "source": [
        "**Ejercicio 2:\n",
        "Genere una consulta que liste los paises cuyo 'ORIGIN_COUNTRY_NAME' no sea Romania, Ireland ni United States**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJa00xjF_KJm",
        "outputId": "3863e55d-605b-44be-e36d-d483902fbfa8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Resolver aqui\n",
        "dfsParquet.where(~F.col('ORIGIN_COUNTRY_NAME').isin('United States', 'Romania', 'Ireland')).show(5)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|    United States|              India|   69|\n",
            "|    United States|          Singapore|   25|\n",
            "|    United States|            Grenada|   54|\n",
            "|    United States|   Marshall Islands|   44|\n",
            "|    United States|       Sint Maarten|   53|\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Resolver aqui\n",
        "dfsParquet.where(\"ORIGIN_COUNTRY_NAME not in ('United States', 'Romania', 'Ireland')\").show(5)"
      ],
      "metadata": {
        "id": "0MoS8rAIiN2x",
        "outputId": "c87c4e9e-d98d-47fc-f958-44636f7a45f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|    United States|              India|   69|\n",
            "|    United States|          Singapore|   25|\n",
            "|    United States|            Grenada|   54|\n",
            "|    United States|   Marshall Islands|   44|\n",
            "|    United States|       Sint Maarten|   53|\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixszkgfefiA0"
      },
      "source": [
        "\n",
        "\n",
        "__Combinación de filtros (AND / OR)__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUE4mQsVASzV"
      },
      "source": [
        "__AND__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeNC6fC5fiA0",
        "outputId": "24319ca1-579c-4019-e358-d2703afa7c47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Forma 1\n",
        "dfsParquet.where( ( F.col('DEST_COUNTRY_NAME') == 'Malta' ) & \n",
        "                 ( F.col('ORIGIN_COUNTRY_NAME') == 'United States' ) ).show(5)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|            Malta|      United States|    1|\n",
            "+-----------------+-------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-kkD3_XfiA2",
        "outputId": "0b90241a-2355-45be-bdda-2105e90bea33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Forma 2\n",
        "dfsParquet.where(\"DEST_COUNTRY_NAME = 'Malta' and ORIGIN_COUNTRY_NAME = 'United States'\").show(5)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|            Malta|      United States|    1|\n",
            "+-----------------+-------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV0rtyXqAVui"
      },
      "source": [
        "__OR__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3l2-sA6AY8S",
        "outputId": "027c368c-17d3-4f0a-a948-070578693d4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Forma 1\n",
        "dfsParquet.where((F.col('DEST_COUNTRY_NAME') == 'Malta') | (F.col('ORIGIN_COUNTRY_NAME') == 'United States')).show(5)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|            Egypt|      United States|   24|\n",
            "|Equatorial Guinea|      United States|    1|\n",
            "|       Costa Rica|      United States|  477|\n",
            "|          Senegal|      United States|   29|\n",
            "|           Guyana|      United States|   17|\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNWFq0w7AY8W",
        "outputId": "2793d638-e33f-4d15-ab74-bf15e2c29452",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Forma 2\n",
        "dfsParquet.where(\"DEST_COUNTRY_NAME = 'Malta' or ORIGIN_COUNTRY_NAME = 'United States'\").show(5)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|            Egypt|      United States|   24|\n",
            "|Equatorial Guinea|      United States|    1|\n",
            "|       Costa Rica|      United States|  477|\n",
            "|          Senegal|      United States|   29|\n",
            "|           Guyana|      United States|   17|\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQuWV0X4A6Lj"
      },
      "source": [
        "**Ejercicio 3:\n",
        "Genere una consulta que liste los paises cuyo 'DEST_COUNTRY_NAME' sea United States y su 'ORIGIN_COUNTRY_NAME' no sea India**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnkShSQmBHuF",
        "outputId": "4465229c-565a-4cbd-f93f-c3ff5d15580a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Resolver aqui\n",
        "dfsParquet.where(\"DEST_COUNTRY_NAME = 'United States' and ORIGIN_COUNTRY_NAME != 'India'\").show(5)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|    United States|            Romania|    1|\n",
            "|    United States|            Ireland|  264|\n",
            "|    United States|          Singapore|   25|\n",
            "|    United States|            Grenada|   54|\n",
            "|    United States|   Marshall Islands|   44|\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQ3yb4E4fiBV"
      },
      "source": [
        "## 5.3. Group By\n",
        "\n",
        "Resume los registros en gru"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "XfWICXRjfiBV",
        "outputId": "096e58fd-0b19-4159-80c1-42313eceac95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dfsCSV.show(5)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|    United States|            Romania|   15|\n",
            "|    United States|            Croatia|    1|\n",
            "|    United States|            Ireland|  344|\n",
            "|            Egypt|      United States|   15|\n",
            "|    United States|              India|   62|\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SELECT DEST_COUNTRY_NAME, count(*)\n",
        "# FROM dfsCSV\n",
        "# WHERE DEST_COUNTRY_NAME = \"United States\"\n",
        "# GROUP BY DEST_COUNTRY_NAME "
      ],
      "metadata": {
        "id": "4g-przN-lWq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onVemclgfiBX",
        "outputId": "40f99c7f-3c97-43bf-886e-fe8181d5c237",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Count\n",
        "dfsCSV.where('DEST_COUNTRY_NAME = \"United States\"').groupBy('DEST_COUNTRY_NAME').count().show(5)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-----+\n",
            "|DEST_COUNTRY_NAME|count|\n",
            "+-----------------+-----+\n",
            "|    United States|  125|\n",
            "+-----------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SELECT DEST_COUNTRY_NAME, count(*)\n",
        "# FROM dfsCSV\n",
        "# GROUP BY DEST_COUNTRY_NAME "
      ],
      "metadata": {
        "id": "QwXZGTL0l6J0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWnyeH-yfiBa",
        "outputId": "dcea04a3-75ee-4c6f-d926-109add640633",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# SUM\n",
        "dfsCSV.groupBy('DEST_COUNTRY_NAME')\\\n",
        ".agg(F.count('*'), F.sum('count')).show(5)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+--------+----------+\n",
            "|DEST_COUNTRY_NAME|count(1)|sum(count)|\n",
            "+-----------------+--------+----------+\n",
            "|         Anguilla|       1|      41.0|\n",
            "|           Russia|       1|     176.0|\n",
            "|         Paraguay|       1|      60.0|\n",
            "|          Senegal|       1|      40.0|\n",
            "|           Sweden|       1|     118.0|\n",
            "+-----------------+--------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6Uqva7ufiBc",
        "outputId": "d4a86ec9-934f-42d0-d158-5087308d0000",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Combinando las dos formas\n",
        "# SELECT count(*), SUM(variable)\n",
        "# FROM dfsCSV\n",
        "# GROUP BY DEST_COUNTRY_NAME\n",
        "\n",
        "dfsCSV.groupBy('DEST_COUNTRY_NAME').agg(F.count('*'), F.max('count')).show(5)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+--------+----------+\n",
            "|DEST_COUNTRY_NAME|count(1)|max(count)|\n",
            "+-----------------+--------+----------+\n",
            "|         Anguilla|       1|        41|\n",
            "|         Paraguay|       1|        60|\n",
            "|           Russia|       1|       176|\n",
            "|          Senegal|       1|        40|\n",
            "|           Sweden|       1|       118|\n",
            "+-----------------+--------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2vVelZ8DQZg"
      },
      "source": [
        "**Ejercicio 4:\n",
        "Genere una consulta que agrupe por 'DEST_COUNTRY_NAME' y saque el minimo, maximo y el prmedio de 'count'**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5s7li2wDcF0",
        "outputId": "de56a9f9-33df-4670-a01e-98e30745d1f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Resuelva aqui\n",
        "dfsCSV.groupBy('DEST_COUNTRY_NAME')\\\n",
        ".agg(F.count('*'),F.countDistinct('ORIGIN_COUNTRY_NAME'),F.avg('count'), F.sum('count'), F.max('count'), F.min('count')).show(5)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+--------+-----------------------------------+----------+----------+----------+----------+\n",
            "|DEST_COUNTRY_NAME|count(1)|count(DISTINCT ORIGIN_COUNTRY_NAME)|avg(count)|sum(count)|max(count)|min(count)|\n",
            "+-----------------+--------+-----------------------------------+----------+----------+----------+----------+\n",
            "|         Anguilla|       1|                                  1|      41.0|      41.0|        41|        41|\n",
            "|         Paraguay|       1|                                  1|      60.0|      60.0|        60|        60|\n",
            "|           Russia|       1|                                  1|     176.0|     176.0|       176|       176|\n",
            "|          Senegal|       1|                                  1|      40.0|      40.0|        40|        40|\n",
            "|           Sweden|       1|                                  1|     118.0|     118.0|       118|       118|\n",
            "+-----------------+--------+-----------------------------------+----------+----------+----------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubebAP_RfiBN"
      },
      "source": [
        "## 5.4. Sort - OrderBy\n",
        "\n",
        "Ordena los registros segun columna."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rLd3PnpENjn",
        "outputId": "4f83069b-ca29-4b70-8814-9a692075431b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dfsMorosidad.show(5)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+-----+-------+-------+-----+-------------+----+-----+\n",
            "| ID|edad|meses|max_ant|ingreso|score|tipo_vivienda|zona|nivel|\n",
            "+---+----+-----+-------+-------+-----+-------------+----+-----+\n",
            "|  1|  28|  122|      9|1115.09|  206|            4|   1|    5|\n",
            "|  2|  27|  173|     61| 2450.0|  205|            2|   1|    5|\n",
            "|  3|  37|  185|      4| 1390.0|  177|            2|   3|    5|\n",
            "|  4|  34|  262|     20| 2200.0|  216|            2|   1|    2|\n",
            "|  5|  40|  150|     17|10000.0|  185|            2|   1|    2|\n",
            "+---+----+-----+-------+-------+-----+-------------+----+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaEyjhAvfiBO",
        "outputId": "9015d550-71d5-499d-c742-ae268140d50f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dfsMorosidad.orderBy(F.col('edad').desc()).show(5)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+-----+-------+-------+-----+-------------+----+-----+\n",
            "| ID|edad|meses|max_ant|ingreso|score|tipo_vivienda|zona|nivel|\n",
            "+---+----+-----+-------+-------+-----+-------------+----+-----+\n",
            "|133|  65|  135|      0|28753.0|  254|            4|   1|    1|\n",
            "| 61|  63|  270|     86| 9180.0|  213|            4|   1|    2|\n",
            "| 92|  61|  122|      6| 9625.0|  238|            2|   1|    2|\n",
            "|275|  55|  125|      8| 3686.3|  211|            2|   1|    5|\n",
            "|152|  55|  162|     23|5593.21|  199|            4|   3|    5|\n",
            "+---+----+-----+-------+-------+-----+-------------+----+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyaJ1FlDfiBQ",
        "outputId": "8f244040-08e9-4506-bc17-77aebdb1934f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dfsMorosidad.sort(F.col('ingreso').asc()).show(5)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+-----+-------+-------+-----+-------------+----+-----+\n",
            "| ID|edad|meses|max_ant|ingreso|score|tipo_vivienda|zona|nivel|\n",
            "+---+----+-----+-------+-------+-----+-------------+----+-----+\n",
            "|117|  27|  121|      0| 608.03|  188|            4|   1|    5|\n",
            "| 72|  24|  120|      0| 608.58|  192|            4|   1|    1|\n",
            "|118|  22|  121|      0| 628.97|  156|            2|   1|    5|\n",
            "|289|  28|  124|      6| 646.25|  204|            2|   1|    5|\n",
            "|227|  26|  134|      0| 664.37|  188|            4|   2|    5|\n",
            "+---+----+-----+-------+-------+-----+-------------+----+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vff-DGYofiBT",
        "outputId": "19602ab0-e507-43f9-bcf6-15b4e382f278",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dfsMorosidad.sort(F.col('tipo_vivienda').asc(), F.col('ingreso').desc()).show(5)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+-----+-------+--------+-----+-------------+----+-----+\n",
            "| ID|edad|meses|max_ant| ingreso|score|tipo_vivienda|zona|nivel|\n",
            "+---+----+-----+-------+--------+-----+-------------+----+-----+\n",
            "| 50|  27|  185|     12|  4200.0|  179|            1|   1|    2|\n",
            "| 47|  27|  136|      8|  3500.0|  200|            1|   3|    2|\n",
            "|110|  33|  150|      0|  2100.0|  182|            1|   4|    2|\n",
            "| 91|  26|  117|      7| 30000.0|  206|            2|   1|    2|\n",
            "| 19|  37|  162|      9|20285.08|  201|            2|   1|    2|\n",
            "+---+----+-----+-------+--------+-----+-------------+----+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m40V_psAT_zT"
      },
      "source": [
        "## Funciones Extras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgHgjlEVT_zW"
      },
      "source": [
        "### Clausula When"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "HBZsvts7T_zW"
      },
      "source": [
        "dfsCSV.select('count', F.when(F.col('count') < 25, \"Menores a 25\").otherwise(\"Mayores a 25\")).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vX3f9OsVT_zY"
      },
      "source": [
        "dfsCSV.select('DEST_COUNTRY_NAME', F.when(F.col('count') < 25, \"Menores a 25\").when(F.col('count') < 50, \"Menores a 50\").otherwise(\"Mayores a 100\")).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVwhh_jcT_zZ"
      },
      "source": [
        "### Clausula Like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6c1Wb28T_za"
      },
      "source": [
        "dfsCSV.select(\"DEST_COUNTRY_NAME\", F.col(\"DEST_COUNTRY_NAME\").like(\"%Egy%\")).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V6yo6XgT_zd"
      },
      "source": [
        "### Clausula Startswith - Endswith"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCmPfY9mT_ze"
      },
      "source": [
        "dfsCSV.select(\"DEST_COUNTRY_NAME\", F.col(\"DEST_COUNTRY_NAME\").startswith(\"Uni\")).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "HKW3Y-GAT_zf"
      },
      "source": [
        "dfsCSV.select(\"DEST_COUNTRY_NAME\", F.col(\"DEST_COUNTRY_NAME\").endswith(\"a\")).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gjz_fbXrT_zj"
      },
      "source": [
        "### Clausula Substring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LkQH2WmT_zk"
      },
      "source": [
        "dfsCSV.select(\"DEST_COUNTRY_NAME\", (F.col(\"DEST_COUNTRY_NAME\").substr(2,4)).alias(\"Nombre_Corto\")).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXZj-w96T_zl"
      },
      "source": [
        "### Clausula Between"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfwS_37tT_zl"
      },
      "source": [
        "dfsCSV.select(\"DEST_COUNTRY_NAME\", \"count\", F.col(\"count\").between(25,75)).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19dzyPnrG1bt"
      },
      "source": [
        "dfsCSV.select(\"DEST_COUNTRY_NAME\", \"count\").where(F.col(\"count\").between(25,75)).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05oQP-2xfiBC"
      },
      "source": [
        "### Distinct\n",
        "\n",
        "Una llamada al método `distinct` es lo mismo que al método `dropDuplicates` sin parámetro. Es decir, tiene en cuenta todas las columnas. También se utiliza normalmente para contar los valores únicos de una columna."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70W-Bd--fiBC"
      },
      "source": [
        "# Usamos la funcion distinct\n",
        "dfsCSV.select('DEST_COUNTRY_NAME').distinct().show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-uQlZdXlZHF"
      },
      "source": [
        "### Agregando Columnas\n",
        "\n",
        "Usamos la sentencia **withColumn()**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6ma41GhlxFE"
      },
      "source": [
        "dfsCSV = dfsCSV.withColumn('Conteo', F.col('count') + 5 )\n",
        "dfsCSV.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wou5UCRkT_zm"
      },
      "source": [
        "### Modificando nombres de columnas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1fwbwiPT_zm"
      },
      "source": [
        "dfsCSV = dfsCSV.withColumnRenamed('count', 'cuenta')\n",
        "dfsCSV.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwyKH_Cwm38P"
      },
      "source": [
        "**Ejercicio 5:\n",
        "Genere una nueva columna llamada 'INICIALES' que sean las tres primeras letras del campo 'DEST_COUNTRY_NAME'**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUHH1gOQm38X"
      },
      "source": [
        "# Resuelva aqui\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7D7ACXznf9g"
      },
      "source": [
        "**Ejercicio 6:\n",
        "Genere una nueva columna llamada 'FLAG_CONTEO' que si la variable cuenta es mayor a 25 entonces tome el valor de 1 caso contrario el valor 0**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIHE_n5IngLm"
      },
      "source": [
        "# Resuelva aqui\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmMWPPvrfiAE"
      },
      "source": [
        "\n",
        "\n",
        "### Drop\n",
        "\n",
        "El método `drop` tiene la función contraria al `select`, elimina un subconjunto de columnas. En este caso no se puede pasar una lista de columnas, es necesario utlizar el operador `*` para convertirlo a parámetros indivuales.\n",
        "\n",
        "**OJO:** Si se intenta eliminar una columna que no existe no devuelve error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXNmi6h8fiAF"
      },
      "source": [
        "# Forma 1\n",
        "dfsCSV = dfsCSV.drop('count','conteo')\n",
        "dfsCSV.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwqCiqEDoSVT"
      },
      "source": [
        "# Forma 2\n",
        "columnas = ['DEST_COUNTRY_NAME','ORIGIN_COUNTRY_NAME']\n",
        "dfsCSV = dfsCSV.drop(*columnas)\n",
        "dfsCSV.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}