{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SparkSQL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JhonFiUNFV/python_prep/blob/master/SparkSQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQgCzt-8UrH6"
      },
      "source": [
        "# 1.- Configuracion Entorno Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdKw7_n8UEX9"
      },
      "source": [
        "# Descargamos spark con hadoop y Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwhBfxJaUq-Z"
      },
      "source": [
        "# Seteamos las variables de entorno\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.7-bin-hadoop2.7\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgrBPLJeVJzO"
      },
      "source": [
        "# Creamos la conexion a Spark \n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpuyGonolr5x"
      },
      "source": [
        "spark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFeRjB5XVhTY"
      },
      "source": [
        "# 2.- Importacion de Librerias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kax8lSkCVnFL"
      },
      "source": [
        "# Librerias Spark\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "#Librerias Python\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPcoVuvlVmRj"
      },
      "source": [
        "# 3.- Importacion de Datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsc40jsIVwY0"
      },
      "source": [
        "# Usamos la librería files de colab\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# También se puede importar desde el mismo navegador"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqZ_B3bEmCt7"
      },
      "source": [
        "## 3.1. Importacion desde un csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jm9I-lybmVuZ"
      },
      "source": [
        "Para importar datos desde un csv utilizamos el comando **spark.read.csv**\n",
        "\n",
        "Sintaxis más utilizada:\n",
        "**dfsSpark = spark.read.csv(DATA_PATH + file_name, sep = ',', header=True, inferSchema=True)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_a8O8metmBPv"
      },
      "source": [
        "help(spark.read.csv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoyoNW3eqL9b"
      },
      "source": [
        "Ejercicio 1:\n",
        "Importar el fichero **salario.csv** y ponerle de nombre dfsSalario"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-d-3aToqmNC"
      },
      "source": [
        "# Insertar codigo aqui\n",
        "dfsCSV = spark.read.csv('2015-summary.csv', sep = ',', header=True, inferSchema=True)\n",
        "dfsCSV.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgOEAnGfq6up"
      },
      "source": [
        "## 3.2. Importacion desde un json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEyUes4Jq6ur"
      },
      "source": [
        "Para importar datos desde un csv utilizamos el comando **spark.read.json**\n",
        "\n",
        "Sintaxis más utilizada:\n",
        "**dfsSpark = spark.read.json(DATA_PATH + file_name)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rf1Scryyq6us"
      },
      "source": [
        "help(spark.read.json)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFcX4Hawq6uy"
      },
      "source": [
        "# Importando desde json\n",
        "dfsJson = spark.read.json('2015-summary.json')\n",
        "dfsJson.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_43sW1Fzr_fv"
      },
      "source": [
        "## 3.3. Importacion desde un txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEfcw8ZTr_fx"
      },
      "source": [
        "Para importar datos desde un csv utilizamos el comando **spark.read.text**\n",
        "\n",
        "Sintaxis más utilizada:\n",
        "**dfsSpark = spark.read.text(DATA_PATH + file_name)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dav3zfPBr_fy"
      },
      "source": [
        "help(spark.read.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ej2dZ_wkr_f3"
      },
      "source": [
        "# Importando desde txt\n",
        "dfsTxt = spark.read.text('salario.txt')\n",
        "dfsTxt.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXnXJU3nsj8Z"
      },
      "source": [
        "## 3.4. Importacion desde un parquet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxbn4xdlsj8b"
      },
      "source": [
        "Para importar datos desde un csv utilizamos el comando **spark.read.parquet**\n",
        "\n",
        "Sintaxis más utilizada:\n",
        "**dfsSpark = spark.read.parquet(DATA_PATH + file_name)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jab0_6Mxsj8c"
      },
      "source": [
        "help(spark.read.parquet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyN6Ky0-sj8i"
      },
      "source": [
        "# Importando desde parquet\n",
        "dfsParquet = spark.read.parquet('part-r-00000-1a9822ba-b8fb-4d8e-844a-ea30d0801b9e.gz.parquet')\n",
        "dfsParquet.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gie7H3JFuAT2"
      },
      "source": [
        "## 3.5. Importacion desde un pandas Dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE_rdmS_uAT9"
      },
      "source": [
        "Para importar datos desde un csv utilizamos el comando **spark.createDataFrame()**\n",
        "\n",
        "Sintaxis más utilizada:\n",
        "**dfsSpark = spark.createDataFrame(pandasDataframe)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-Q-VW3MuAT-"
      },
      "source": [
        "help(spark.createDataFrame)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mxlEVs4uAUE"
      },
      "source": [
        "# Importando desde csv\n",
        "dfpMorosidad = pd.read_csv('morosidad.csv')\n",
        "dfpMorosidad.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18E_-3j4u-FH"
      },
      "source": [
        "# Verificando tipo de dato\n",
        "type(dfpMorosidad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIx7vIchvKgL"
      },
      "source": [
        "# Convirtiendo a Spark Dataframe\n",
        "dfsMorosidad = spark.createDataFrame(dfpMorosidad)\n",
        "dfsMorosidad.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJEjP3aEvYqH"
      },
      "source": [
        "# Verificando tipo de dato\n",
        "type(dfsMorosidad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05HR1FDjXlAp"
      },
      "source": [
        "# 4.- Información básica de DataFrames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnjOnHpeXlAq"
      },
      "source": [
        "\n",
        "\n",
        "## 4.1. Previsualización\n",
        "\n",
        "**show** es un método que muestra por pantalla _n_ filas del DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bST3B8a3XlAs"
      },
      "source": [
        "dfsCSV.show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ovMewSLXlAz"
      },
      "source": [
        "\n",
        "\n",
        "## 4.2. Dimensiones\n",
        "\n",
        "En Spark, no existe un método *shape*, por lo que hay que contar por separados las filas y las columnas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxCcILDnXlA0"
      },
      "source": [
        "# Contando el numero de filas\n",
        "dfsCSV.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OfUfptoXlA4"
      },
      "source": [
        "\n",
        "\n",
        "`columns` es un atributo que contiene los nombres de las columnas del DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "h6jfIv3-XlA5"
      },
      "source": [
        "dfsMorosidad.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAMq6fuAXlA-"
      },
      "source": [
        "len(dfsMorosidad.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu11X1KAXlBB"
      },
      "source": [
        "\n",
        "\n",
        "## 4.3. Schema\n",
        "\n",
        "El schema de un dataframe nos muestra como se interpretaran los datos. Esto no significa que los datos estén así. _schema_ es un atributo del objeto, no un método. _printSchema()_ es un método que muestra una versión más entendidible del _schema_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HlSGfQnXlBD"
      },
      "source": [
        "dfsCSV.schema"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HS4KFHPHXlBL"
      },
      "source": [
        "dfsCSV.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEZY2FO8XlBO"
      },
      "source": [
        "\n",
        "\n",
        "## 4.3. dtypes\n",
        "\n",
        "El atributo `dtypes` contiene los nombres de las columnas del dataframe junto con su tipo. Esto permite seleccionar nombres de columnas basados en el tipo, normalmente las variables categóricas (string y boolean) tienen tratamientos distintos a las numéricas (enteras y decimales)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fo7TO5tQXlBO"
      },
      "source": [
        "dfsCSV.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IbXVDXv5lbJ"
      },
      "source": [
        "# 5.- Operaciones con Dataframes de Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jS0OkGny6kky"
      },
      "source": [
        "dfsMorosidad.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2zeHlU8fiAA"
      },
      "source": [
        "## 5.1. Select\n",
        "\n",
        "Filtra las columnas que deseamos mostrar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_kl-qPufiAA"
      },
      "source": [
        "# Forma 1\n",
        "dfsMorosidad.select('meses', 'score', 'zona').show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XC_0lQ_6uUk"
      },
      "source": [
        "# Forma 2\n",
        "dfsMorosidad.select(F.col('meses'), F.col('meses') * 2, F.col('score'), F.col('zona')).show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fr-8GPWgfiAC"
      },
      "source": [
        "# Forma 3\n",
        "columnas = ['meses','score','zona']\n",
        "dfsMorosidad.select(columnas).show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdPJYFuX7ISt"
      },
      "source": [
        "# Combinando las tres formas\n",
        "columnas = ['meses','score','zona']\n",
        "dfsMorosidad.select('ID', F.col('nivel'), *columnas).show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QseBhxTfiAf"
      },
      "source": [
        "## 5.2. Filter - Where\n",
        "\n",
        "filtra resgistros segun cumplan la condicion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSgvCxey8Iaz"
      },
      "source": [
        "dfsParquet.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJmYt80xfiAg"
      },
      "source": [
        "# Forma 1\n",
        "dfsParquet.where(\"DEST_COUNTRY_NAME = 'United States'\").show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RphjZAzqfiAi"
      },
      "source": [
        "# Forma 2\n",
        "dfsParquet.filter(F.col('ORIGIN_COUNTRY_NAME') == 'India').select('ORIGIN_COUNTRY_NAME','count').show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d99lS1tD8uKm"
      },
      "source": [
        "**Ejercicio 1:\n",
        "Genere una consulta que liste los paises cuyo 'count' sea mayor a 25**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LMgFhbPfiAl"
      },
      "source": [
        "# Resolver aqui\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_Ct_whdfiAv"
      },
      "source": [
        "# Varios valores - Forma 1\n",
        "paises = ['United States', 'Egypt', 'Equatorial Guinea']\n",
        "dfsParquet.where(F.col('DEST_COUNTRY_NAME').isin(paises)).show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDI_z46e96B5"
      },
      "source": [
        "# Varios valores - Forma 2\n",
        "dfsParquet.where(\"DEST_COUNTRY_NAME in ('United States', 'Egypt', 'Equatorial Guinea')\").show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhVE4Dbe-Jt2"
      },
      "source": [
        "# Varios valores - Forma 3\n",
        "dfsParquet.where(F.col('DEST_COUNTRY_NAME').isin('United States', 'Egypt', 'Equatorial Guinea')).show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QNeBiEo-Z2o"
      },
      "source": [
        "**OJO**: Para negar podemos usar el operador **~**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "MlEy1HEtfiAw"
      },
      "source": [
        "# Negamos la anterior sentencia\n",
        "dfsParquet.where(~F.col('DEST_COUNTRY_NAME').isin('United States', 'Egypt', 'Equatorial Guinea')).show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOxOor6l_KJk"
      },
      "source": [
        "**Ejercicio 2:\n",
        "Genere una consulta que liste los paises cuyo 'ORIGIN_COUNTRY_NAME' no sea Romania, Ireland ni United States**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJa00xjF_KJm"
      },
      "source": [
        "# Resolver aqui\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixszkgfefiA0"
      },
      "source": [
        "\n",
        "\n",
        "__Combinación de filtros (AND / OR)__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUE4mQsVASzV"
      },
      "source": [
        "__AND__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeNC6fC5fiA0"
      },
      "source": [
        "# Forma 1\n",
        "dfsParquet.where( ( F.col('DEST_COUNTRY_NAME') == 'Malta' ) & \n",
        "                 ( F.col('ORIGIN_COUNTRY_NAME') == 'United States' ) ).show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-kkD3_XfiA2"
      },
      "source": [
        "# Forma 2\n",
        "dfsParquet.where(\"DEST_COUNTRY_NAME = 'Malta' and ORIGIN_COUNTRY_NAME = 'United States'\").show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV0rtyXqAVui"
      },
      "source": [
        "__OR__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3l2-sA6AY8S"
      },
      "source": [
        "# Forma 1\n",
        "dfsParquet.where((F.col('DEST_COUNTRY_NAME') == 'Malta') | (F.col('ORIGIN_COUNTRY_NAME') == 'United States')).show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNWFq0w7AY8W"
      },
      "source": [
        "# Forma 2\n",
        "dfsParquet.where(\"DEST_COUNTRY_NAME = 'Malta' or ORIGIN_COUNTRY_NAME = 'United States'\").show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQuWV0X4A6Lj"
      },
      "source": [
        "**Ejercicio 3:\n",
        "Genere una consulta que liste los paises cuyo 'DEST_COUNTRY_NAME' sea United States y su 'ORIGIN_COUNTRY_NAME' no sea India**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnkShSQmBHuF"
      },
      "source": [
        "# Resolver aqui\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQ3yb4E4fiBV"
      },
      "source": [
        "## 5.3. Group By\n",
        "\n",
        "Resume los registros en gru"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "XfWICXRjfiBV"
      },
      "source": [
        "dfsCSV.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onVemclgfiBX"
      },
      "source": [
        "# Count\n",
        "dfsCSV.where('DEST_COUNTRY_NAME = \"United States\"').groupBy('DEST_COUNTRY_NAME').count().show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWnyeH-yfiBa"
      },
      "source": [
        "# SUM\n",
        "dfsCSV.groupBy('DEST_COUNTRY_NAME').sum('count').show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6Uqva7ufiBc"
      },
      "source": [
        "# Combinando las dos formas\n",
        "# SELECT count(*), SUM(variable)\n",
        "# FROM dfsCSV\n",
        "# GROUP BY DEST_COUNTRY_NAME\n",
        "\n",
        "dfsCSV.groupBy('DEST_COUNTRY_NAME').agg(F.count('*'), F.max('count')).show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2vVelZ8DQZg"
      },
      "source": [
        "**Ejercicio 4:\n",
        "Genere una consulta que agrupe por 'DEST_COUNTRY_NAME' y saque el minimo, maximo y el prmedio de 'count'**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5s7li2wDcF0"
      },
      "source": [
        "# Resuelva aqui\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubebAP_RfiBN"
      },
      "source": [
        "## 5.4. Sort - OrderBy\n",
        "\n",
        "Ordena los registros segun columna."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rLd3PnpENjn"
      },
      "source": [
        "dfsMorosidad.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaEyjhAvfiBO"
      },
      "source": [
        "dfsMorosidad.orderBy(F.col('edad').desc()).show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyaJ1FlDfiBQ"
      },
      "source": [
        "dfsMorosidad.sort(F.col('ingreso').asc()).show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vff-DGYofiBT"
      },
      "source": [
        "dfsMorosidad.sort(F.col('tipo_vivienda').asc(), F.col('ingreso').desc()).show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m40V_psAT_zT"
      },
      "source": [
        "## Funciones Extras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgHgjlEVT_zW"
      },
      "source": [
        "### Clausula When"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "HBZsvts7T_zW"
      },
      "source": [
        "dfsCSV.select('count', F.when(F.col('count') < 25, \"Menores a 25\").otherwise(\"Mayores a 25\")).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vX3f9OsVT_zY"
      },
      "source": [
        "dfsCSV.select('DEST_COUNTRY_NAME', F.when(F.col('count') < 25, \"Menores a 25\").when(F.col('count') < 50, \"Menores a 50\").otherwise(\"Mayores a 100\")).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVwhh_jcT_zZ"
      },
      "source": [
        "### Clausula Like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6c1Wb28T_za"
      },
      "source": [
        "dfsCSV.select(\"DEST_COUNTRY_NAME\", F.col(\"DEST_COUNTRY_NAME\").like(\"%Egy%\")).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V6yo6XgT_zd"
      },
      "source": [
        "### Clausula Startswith - Endswith"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCmPfY9mT_ze"
      },
      "source": [
        "dfsCSV.select(\"DEST_COUNTRY_NAME\", F.col(\"DEST_COUNTRY_NAME\").startswith(\"Uni\")).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "HKW3Y-GAT_zf"
      },
      "source": [
        "dfsCSV.select(\"DEST_COUNTRY_NAME\", F.col(\"DEST_COUNTRY_NAME\").endswith(\"a\")).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gjz_fbXrT_zj"
      },
      "source": [
        "### Clausula Substring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LkQH2WmT_zk"
      },
      "source": [
        "dfsCSV.select(\"DEST_COUNTRY_NAME\", (F.col(\"DEST_COUNTRY_NAME\").substr(2,4)).alias(\"Nombre_Corto\")).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXZj-w96T_zl"
      },
      "source": [
        "### Clausula Between"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfwS_37tT_zl"
      },
      "source": [
        "dfsCSV.select(\"DEST_COUNTRY_NAME\", \"count\", F.col(\"count\").between(25,75)).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19dzyPnrG1bt"
      },
      "source": [
        "dfsCSV.select(\"DEST_COUNTRY_NAME\", \"count\").where(F.col(\"count\").between(25,75)).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05oQP-2xfiBC"
      },
      "source": [
        "### Distinct\n",
        "\n",
        "Una llamada al método `distinct` es lo mismo que al método `dropDuplicates` sin parámetro. Es decir, tiene en cuenta todas las columnas. También se utiliza normalmente para contar los valores únicos de una columna."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70W-Bd--fiBC"
      },
      "source": [
        "# Usamos la funcion distinct\n",
        "dfsCSV.select('DEST_COUNTRY_NAME').distinct().show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-uQlZdXlZHF"
      },
      "source": [
        "### Agregando Columnas\n",
        "\n",
        "Usamos la sentencia **withColumn()**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6ma41GhlxFE"
      },
      "source": [
        "dfsCSV = dfsCSV.withColumn('Conteo', F.col('count') + 5 )\n",
        "dfsCSV.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wou5UCRkT_zm"
      },
      "source": [
        "### Modificando nombres de columnas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1fwbwiPT_zm"
      },
      "source": [
        "dfsCSV = dfsCSV.withColumnRenamed('count', 'cuenta')\n",
        "dfsCSV.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwyKH_Cwm38P"
      },
      "source": [
        "**Ejercicio 5:\n",
        "Genere una nueva columna llamada 'INICIALES' que sean las tres primeras letras del campo 'DEST_COUNTRY_NAME'**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUHH1gOQm38X"
      },
      "source": [
        "# Resuelva aqui\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7D7ACXznf9g"
      },
      "source": [
        "**Ejercicio 6:\n",
        "Genere una nueva columna llamada 'FLAG_CONTEO' que si la variable cuenta es mayor a 25 entonces tome el valor de 1 caso contrario el valor 0**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIHE_n5IngLm"
      },
      "source": [
        "# Resuelva aqui\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmMWPPvrfiAE"
      },
      "source": [
        "\n",
        "\n",
        "### Drop\n",
        "\n",
        "El método `drop` tiene la función contraria al `select`, elimina un subconjunto de columnas. En este caso no se puede pasar una lista de columnas, es necesario utlizar el operador `*` para convertirlo a parámetros indivuales.\n",
        "\n",
        "**OJO:** Si se intenta eliminar una columna que no existe no devuelve error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXNmi6h8fiAF"
      },
      "source": [
        "# Forma 1\n",
        "dfsCSV = dfsCSV.drop('count','conteo')\n",
        "dfsCSV.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwqCiqEDoSVT"
      },
      "source": [
        "# Forma 2\n",
        "columnas = ['DEST_COUNTRY_NAME','ORIGIN_COUNTRY_NAME']\n",
        "dfsCSV = dfsCSV.drop(*columnas)\n",
        "dfsCSV.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}